{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer Learning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cleysonl/Pytorch/blob/master/Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN84AGg_pciG",
        "colab_type": "text"
      },
      "source": [
        "# **Transfer Learning: Ants and Bees with PyTorch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ExZsupEgUNN",
        "colab_type": "code",
        "outputId": "976ec2b8-d341-4cf6-e263-114829dcccfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "!pip3 install torch torchvision\n",
        "!pip install Pillow==4.0.0"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.0+cu100)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.1+cu100)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.3)\n",
            "Collecting pillow>=4.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/5c/0e94e689de2476c4c5e644a3bd223a1c1b9e2bdb7c510191750be74fa786/Pillow-6.2.1-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pillow\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-6.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting Pillow==4.0.0\n",
            "  Using cached https://files.pythonhosted.org/packages/37/e8/b3fbf87b0188d22246678f8cd61e23e31caa1769ebc06f1664e2e5fe8a17/Pillow-4.0.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow==4.0.0) (0.46)\n",
            "\u001b[31mERROR: torchvision 0.4.1+cu100 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: scikit-image 0.15.0 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 6.2.1\n",
            "    Uninstalling Pillow-6.2.1:\n",
            "      Successfully uninstalled Pillow-6.2.1\n",
            "Successfully installed Pillow-4.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhz9RbJ3gmrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms, models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlzzZNk-g3zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I63l95HXhHYl",
        "colab_type": "code",
        "outputId": "96580f7c-2520-462b-db8e-d3015b1e9389",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/jaddoescad/ants_and_bees.git"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'ants_and_bees' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqVhVGRChK8n",
        "colab_type": "code",
        "outputId": "e0562424-c545-4710-b9e9-35874d27e5df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!ls ants_and_bees/train/ants"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 0013035.jpg\t\t     408393566_b5b694119b.jpg\n",
            " 1030023514_aad5c608f9.jpg   424119020_6d57481dab.jpg\n",
            " 1095476100_3906d8afde.jpg   424873399_47658a91fb.jpg\n",
            " 1099452230_d1949d3250.jpg   450057712_771b3bfc91.jpg\n",
            " 116570827_e9c126745d.jpg    45472593_bfd624f8dc.jpg\n",
            " 1225872729_6f0856588f.jpg   459694881_ac657d3187.jpg\n",
            " 1262877379_64fcada201.jpg   460372577_f2f6a8c9fc.jpg\n",
            " 1269756697_0bce92cdab.jpg   460874319_0a45ab4d05.jpg\n",
            " 1286984635_5119e80de1.jpg   466430434_4000737de9.jpg\n",
            " 132478121_2a430adea2.jpg    470127037_513711fd21.jpg\n",
            " 1360291657_dc248c5eea.jpg   474806473_ca6caab245.jpg\n",
            " 1368913450_e146e2fb6d.jpg   475961153_b8c13fd405.jpg\n",
            " 1473187633_63ccaacea6.jpg   484293231_e53cfc0c89.jpg\n",
            " 148715752_302c84f5a4.jpg    49375974_e28ba6f17e.jpg\n",
            " 1489674356_09d48dde0a.jpg   506249802_207cd979b4.jpg\n",
            " 149244013_c529578289.jpg    506249836_717b73f540.jpg\n",
            " 150801003_3390b73135.jpg    512164029_c0a66b8498.jpg\n",
            " 150801171_cd86f17ed8.jpg    512863248_43c8ce579b.jpg\n",
            " 154124431_65460430f2.jpg    518773929_734dbc5ff4.jpg\n",
            " 162603798_40b51f1654.jpg    522163566_fec115ca66.jpg\n",
            " 1660097129_384bf54490.jpg   522415432_2218f34bf8.jpg\n",
            " 167890289_dd5ba923f3.jpg    531979952_bde12b3bc0.jpg\n",
            " 1693954099_46d4c20605.jpg   533848102_70a85ad6dd.jpg\n",
            " 175998972.jpg\t\t     535522953_308353a07c.jpg\n",
            " 178538489_bec7649292.jpg    540889389_48bb588b21.jpg\n",
            " 1804095607_0341701e1c.jpg   541630764_dbd285d63c.jpg\n",
            " 1808777855_2a895621d7.jpg   543417860_b14237f569.jpg\n",
            " 188552436_605cc9b36b.jpg    560966032_988f4d7bc4.jpg\n",
            " 1917341202_d00a7f9af5.jpg   5650366_e22b7e1065.jpg\n",
            " 1924473702_daa9aacdbe.jpg   6240329_72c01e663e.jpg\n",
            " 196057951_63bf063b92.jpg    6240338_93729615ec.jpg\n",
            " 196757565_326437f5fe.jpg    649026570_e58656104b.jpg\n",
            " 201558278_fe4caecc76.jpg    662541407_ff8db781e7.jpg\n",
            " 201790779_527f4c0168.jpg    67270775_e9fdf77e9d.jpg\n",
            " 2019439677_2db655d361.jpg   6743948_2b8c096dda.jpg\n",
            " 207947948_3ab29d7207.jpg    684133190_35b62c0c1d.jpg\n",
            " 20935278_9190345f6b.jpg     69639610_95e0de17aa.jpg\n",
            " 224655713_3956f7d39a.jpg    707895295_009cf23188.jpg\n",
            " 2265824718_2c96f485da.jpg   7759525_1363d24e88.jpg\n",
            " 2265825502_fff99cfd2d.jpg   795000156_a9900a4a71.jpg\n",
            " 226951206_d6bf946504.jpg    822537660_caf4ba5514.jpg\n",
            " 2278278459_6b99605e50.jpg   82852639_52b7f7f5e3.jpg\n",
            " 2288450226_a6e96e8fdf.jpg   841049277_b28e58ad05.jpg\n",
            " 2288481644_83ff7e4572.jpg   886401651_f878e888cd.jpg\n",
            " 2292213964_ca51ce4bef.jpg   892108839_f1aad4ca46.jpg\n",
            " 24335309_c5ea483bb8.jpg     938946700_ca1c669085.jpg\n",
            " 245647475_9523dfd13e.jpg    957233405_25c1d1187b.jpg\n",
            " 255434217_1b2b3fe0a4.jpg    9715481_b3cb4114ff.jpg\n",
            " 258217966_d9d90d18d3.jpg    998118368_6ac1d91f81.jpg\n",
            " 275429470_b2d7d9290b.jpg    Ant_1.jpg\n",
            " 28847243_e79fe052cd.jpg    'ant photos.jpg'\n",
            " 318052216_84dff3f98a.jpg    army-ants-red-picture.jpg\n",
            " 334167043_cbd1adaeb9.jpg    formica.jpeg\n",
            " 339670531_94b75ae47a.jpg    hormiga_co_por.jpg\n",
            " 342438950_a3da61deab.jpg    imageNotFound.gif\n",
            " 36439863_0bec9f554f.jpg     kurokusa.jpg\n",
            " 374435068_7eee412ec4.jpg    MehdiabadiAnt2_600.jpg\n",
            " 382971067_0bfd33afe0.jpg    Nepenthes_rafflesiana_ant.jpg\n",
            " 384191229_5779cf591b.jpg    swiss-army-ant.jpg\n",
            " 386190770_672743c9a7.jpg    termite-vs-ant.jpg\n",
            " 392382602_1b7bed32fa.jpg    trap-jaw-ant-insect-bg.jpg\n",
            " 403746349_71384f5b58.jpg    VietnameseAntMimicSpider.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DoMl7XPkRTV",
        "colab_type": "text"
      },
      "source": [
        "## **Transformation and Data Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-Va65MFhS82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform_train = transforms.Compose([transforms.Resize((224,224)),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.RandomRotation(10),\n",
        "                                transforms.RandomAffine(0, shear = 10, scale = (0.8, 1.2)),\n",
        "                                transforms.ColorJitter(brightness = 0.2, contrast = 0.2, saturation = 0.2),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                ])\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize((224,224)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7JkVg-HijF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_dataset = datasets.ImageFolder('ants_and_bees/train', transform = transform_train)\n",
        "validation_dataset = datasets.ImageFolder('ants_and_bees/val', transform = transform)\n",
        "\n",
        "training_loader = torch.utils.data.DataLoader(training_dataset, batch_size= 20, shuffle =True)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size= 20, shuffle =False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBwL-KAvjjF_",
        "colab_type": "text"
      },
      "source": [
        "## **Image Conversion function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmmtjItvjloZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def im_convert(tensor):\n",
        "  image = tensor.cpu().clone().detach().numpy()\n",
        "  image = image.transpose(1,2,0)\n",
        "  # image = image[:,:,0]\n",
        "  image = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n",
        "  image = image.clip(0,1)\n",
        "  return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gvL_E5qkpHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = ('ant', 'bees')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81JXIo0QlBWL",
        "colab_type": "text"
      },
      "source": [
        "## **Initialize the model VGG16**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjewHizslK4z",
        "colab_type": "code",
        "outputId": "b6792229-5d71-4d85-8b6b-0eec557ce290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "# We want the model with the pretrained values\n",
        "model = models.vgg16(pretrained=True)\n",
        "print(model)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-eNf38IolJR",
        "colab_type": "text"
      },
      "source": [
        "## **Define new last_layer for specific case of Transfer Learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK8D7fm8nRQH",
        "colab_type": "code",
        "outputId": "9b7dce67-4cfc-4617-d050-d535a385573c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for param in model.features.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# we want to take the inputs for the 6 layer to train them with our new data\n",
        "n_inputs = model.classifier[6].in_features\n",
        "# we create a new last_layer that has as outputs the classes we want to classify\n",
        "last_layer = nn.Linear(n_inputs, len(classes))\n",
        "model.classifier[6] = last_layer\n",
        "model.to(device)\n",
        "# Check the number of outputs from the last layer\n",
        "print(model.classifier[6].out_features)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPI_-wsDow6U",
        "colab_type": "text"
      },
      "source": [
        "## **Loss and Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEM8gxIYo8fr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G448CopKpNnW",
        "colab_type": "text"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiCOLZm8pSEC",
        "colab_type": "code",
        "outputId": "901e828f-b5f9-421e-a1a1-764348a4ca67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "epochs = 10\n",
        "running_loss_history = []\n",
        "running_corrects_history = []\n",
        "val_running_loss_history = []\n",
        "val_running_corrects_history = []\n",
        " \n",
        "for e in range(epochs):\n",
        "  \n",
        "  running_loss = 0.0\n",
        "  running_corrects = 0.0\n",
        "  val_running_loss = 0.0\n",
        "  val_running_corrects = 0.0\n",
        "  \n",
        "  for inputs, labels in training_loader:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    running_loss += loss.item()\n",
        "    running_corrects += torch.sum(preds == labels.data)\n",
        " \n",
        "  else:\n",
        "    with torch.no_grad():\n",
        "      for val_inputs, val_labels in validation_loader:\n",
        "        val_inputs = val_inputs.to(device)\n",
        "        val_labels = val_labels.to(device)\n",
        "        val_outputs = model(val_inputs)\n",
        "        val_loss = criterion(val_outputs, val_labels)\n",
        "        \n",
        "        _, val_preds = torch.max(val_outputs, 1)\n",
        "        val_running_loss += val_loss.item()\n",
        "        val_running_corrects += torch.sum(val_preds == val_labels.data)\n",
        "      \n",
        "    epoch_loss = running_loss/len(training_loader.dataset)\n",
        "    epoch_acc = running_corrects.float()/ len(training_loader.dataset)\n",
        "    running_loss_history.append(epoch_loss)\n",
        "    running_corrects_history.append(epoch_acc)\n",
        "    \n",
        "    val_epoch_loss = val_running_loss/len(validation_loader.dataset)\n",
        "    val_epoch_acc = val_running_corrects.float()/ len(validation_loader.dataset)\n",
        "    val_running_loss_history.append(val_epoch_loss)\n",
        "    val_running_corrects_history.append(val_epoch_acc)\n",
        "    print('epoch :', (e+1))\n",
        "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
        "    print('validation loss: {:.4f}, validation acc {:.4f} '.format(val_epoch_loss, val_epoch_acc.item()))\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch : 1\n",
            "training loss: 0.1216, acc 0.6639 \n",
            "validation loss: 0.0372, validation acc 0.8301 \n",
            "epoch : 2\n",
            "training loss: 0.0221, acc 0.8934 \n",
            "validation loss: 0.0297, validation acc 0.9085 \n",
            "epoch : 3\n",
            "training loss: 0.0190, acc 0.9139 \n",
            "validation loss: 0.0463, validation acc 0.8954 \n",
            "epoch : 4\n",
            "training loss: 0.0196, acc 0.9385 \n",
            "validation loss: 0.0486, validation acc 0.9216 \n",
            "epoch : 5\n",
            "training loss: 0.0134, acc 0.9467 \n",
            "validation loss: 0.0515, validation acc 0.9216 \n",
            "epoch : 6\n",
            "training loss: 0.0455, acc 0.9057 \n",
            "validation loss: 0.1131, validation acc 0.8627 \n",
            "epoch : 7\n",
            "training loss: 0.0256, acc 0.9508 \n",
            "validation loss: 0.1330, validation acc 0.8562 \n",
            "epoch : 8\n",
            "training loss: 0.0230, acc 0.9467 \n",
            "validation loss: 0.0854, validation acc 0.9020 \n",
            "epoch : 9\n",
            "training loss: 0.0090, acc 0.9672 \n",
            "validation loss: 0.0847, validation acc 0.8954 \n",
            "epoch : 10\n",
            "training loss: 0.0273, acc 0.9590 \n",
            "validation loss: 0.1008, validation acc 0.8954 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aih4205pycX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}